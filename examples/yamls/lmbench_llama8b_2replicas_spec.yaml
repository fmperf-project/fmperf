# LMBenchmark Workload Specification Example
# This specification is used for running benchmarks with the LMBenchmark container
model_name: "meta-llama/Llama-3.1-8B-Instruct"  # Model identifier
scenarios: "all"  # Scenarios to run (all, or sharegpt, long-input, short-input)
# sharegpt: 0.5 0.67 0.84 1 1.17 1.34
# long-input: 1.1 0.9 0.7 0.5 0.3 0.1
# short-input: 0.5 1 2 5 10
qps_values: "0.25 0.5 0.75 1.0 1.25 1.5 1.75 2.0 2.25 2.5 2.75 3.0 3.25 3.5 3.75 4.0 4.25 4.5 4.75 5.0 5.25 5.5 5.75 6.0 6.25 6.5 6.75 7.0 7.25 7.5 7.75 8.0 8.25 8.5 8.75 9.0 9.25 9.5 9.75 10.0"  # Space-separated list of QPS values to test
image: "lmcache/lmcache-benchmark:main"  # Container image to use
service_account: "inference-gateway"  # Service account to use for the job 