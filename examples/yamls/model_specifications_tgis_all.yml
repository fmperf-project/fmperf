name: "google/flan-t5-small"
deployment_framework: "hf_transformers"
dtype_str: "float16"
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "ibm/mpt-7b-instruct2"
deployment_framework: "hf_accelerate"
dtype_str: "float16"
max_batch_size: 64
max_sequence_length: 2048
max_concurrent_requests: 100
max_new_tokens: 1024
max_prefill_weight: 2_000_000
max_batch_weight: 200_000_000
trust_remote_code: true
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "/transformers_cache/LLaMa/models/hf/70B"
shortname: "llama-70b"
deployment_framework: "hf_transformers"
dtype_str: "float16"
max_batch_size: 128
max_sequence_length: 4096
max_concurrent_requests: 160
max_batch_weight: 200_000
max_prefill_weight: 60_000
max_new_tokens: 1536
flash_attention: true
compile: false
num_gpus: 4
cuda_visible_devices: "0,1,2,3"
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "/transformers_cache/LLaMa/models/hf/7B"
shortname: "llama-7b"
deployment_framework: "hf_transformers"
dtype_str: "float16"
max_batch_size: 256
max_sequence_length: 4096
max_concurrent_requests: 320
max_batch_weight: 36_000
max_prefill_weight: 0
max_new_tokens: 1536
flash_attention: true
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "/transformers_cache/LLaMa/models/hf/13B"
shortname: "llama-13b"
deployment_framework: "hf_transformers"
dtype_str: "float16"
max_batch_size: 256
max_sequence_length: 4096
max_concurrent_requests: 320
max_batch_weight: 19_000
max_prefill_weight: 0
max_new_tokens: 1536
flash_attention: true
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "ibm/mpt-7b-instruct2"
deployment_framework: "hf_transformers"
dtype_str: "float16"
max_batch_size: 12
max_sequence_length: 2048
max_concurrent_requests: 64
trust_remote_code: true
compile: true
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "google/flan-t5-xxl"
deployment_framework: "hf_accelerate"
dtype_str: "float16"
max_batch_size: 128
max_sequence_length: 4096
max_concurrent_requests: 200
max_batch_weight: 47_458_400
max_new_tokens: 1536
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "bigscience/mt0-xxl"
deployment_framework: "hf_accelerate"
dtype_str: "float16"
max_batch_size: 128
max_sequence_length: 4096
max_concurrent_requests: 200
max_batch_weight: 44_752_800
max_new_tokens: 1536
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "google/flan-ul2"
deployment_framework: "hf_transformers"
dtype_str: "float16"
max_batch_size: 128
max_sequence_length: 4096
max_concurrent_requests: 200
max_batch_weight: 34_543_200
max_new_tokens: 1536
compile: true
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "google/flan-t5-xxl"
deployment_framework: "hf_accelerate"
dtype_str: "float16"
max_batch_size: 128
max_sequence_length: 4096
max_concurrent_requests: 200
max_batch_weight: 30_000_000
max_new_tokens: 1536
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "google/flan-t5-xl"
deployment_framework: "hf_transformers"
dtype_str: "bfloat16"
max_batch_size: 128
max_sequence_length: 4096
max_concurrent_requests: 200
max_batch_weight: 47_458_400
max_new_tokens: 1536
compile: true
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
---
name: "togethercomputer/gpt-jt-6b-v1"
deployment_framework: "hf_transformers"
dtype_str: "bfloat16"
max_batch_size: 12
max_sequence_length: 2048
max_concurrent_requests: 32
compile: false
image: "quay.io/wxpe/text-gen-server:main.9b4aea8"
